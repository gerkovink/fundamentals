---
title: "Linear modeling and its assumptions"
author: "Gerko Vink"
date: "Fundamental Techniques in Data Science with R"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
---

## Last week

- pipes with package `magrittr`
  - `%>%`
  - `%$%`

- deviations
  - from the mean
  - standardized
  - squared
  
## Today
- the linear model
- assumptions associated with the linear model

##  We use the following packages
```{r message=FALSE}
library(dplyr)
library(magrittr)
library(ggplot2)
library(mice)
```

# Statistical models

## Statistical model

The mathematical formulation of relationship between variables can be written as

\[
\mbox{observed}=\mbox{predicted}+\mbox{error}
\]

or (for the greek people) in notation as
\[y=\mu+\varepsilon\]

where

-   $\mu$ (mean) is the part of $Y$ that is explained by model 
-  $\varepsilon$ (residual) is the part of $Y$ that is not explained by model 


## A simple example
Regression model:

-  Model individual age from weight

\[
\text{age}_i=\alpha+\beta\cdot{\text{weight}}_i+\varepsilon_i
\]

where

-  $\alpha+\beta{x}_i$ is the mean of `age`, $y$, conditional on `weight`, $x$.
-  $\varepsilon_i$ is random variation 

## The linear model

The function `lm()` is a base function in `R` and allows you to pose a variety of linear models. 

```{r}
args(lm)
```

If we want to know what these arguments do we can ask R:

```{r, eval=FALSE}
?lm
```

This will open a help page on the `lm()` function.

## The `boys` data from `mice`
```{r}
head(boys, n = 20)
```

## Continuous predictors {.smaller}
To obtain a linear model with a main effects for `wgt`, we formulate $age\sim wgt$ 
```{r warning=FALSE, message = FALSE}
boys %$% 
  lm(age ~ wgt)
```

## Continuous predictors: more detail {.smaller}
```{r}
boys %$% lm(age ~ wgt) %>% summary()
```

## Continuous predictors
To obtain a linear model with just main effects for `wgt` and `hgt`, we formulate $age\sim wgt + hgt$ 
```{r}
boys %$% 
  lm(age ~ wgt + hgt)
```

## Continuous predictors: more detail {.smaller}
```{r}
boys %$% 
  lm(age ~ wgt + hgt) %>%
  summary
```

## Continuous predictors: interaction effects
To predict $age$ from $wgt$, $hgt$ and the interaction $wgt*hgt$ we formulate $age\sim wgt * hgt$
```{r}
boys %$% lm(age ~ wgt * hgt) %>% summary
```

## Categorical predictors in the linear model
If a categorical variable is entered into function `lm()`, it is automatically converted to a dummy set in `R`. The first level is always taken as the reference category. If we want another reference category we can use the function `relevel()` to change this.

```{r}
fit <- boys %$% 
  lm(age ~ reg)
fit
```

## What are dummies?
```{r}
fit %>% model.matrix() %>% tail(n = 20)
```

##  and again with more detail {.smaller}
```{r}
fit %>% summary
```

## Components of the linear model {.smaller}
```{r}
fit %>% names() # the names of the list with output objects
fit$coef  # show the estimated coefficients
fit %>% coef # alternative
```

# Fitting a line to data

##  Linear regression
Linear regression model
\[
y_i=\alpha+\beta{x}_i+\varepsilon_i
\]

Assumptions:

  -  $y_i$ conditionally normal with mean $\mu_i=\alpha+\beta{x}_i$
  -  $\varepsilon_i$ are $i.i.d.$ with mean 0 and (constant) variance $\sigma^2$

## The `anscombe` data
```{r}
head(anscombe)
```

Or, alternatively, 
```{r}
anscombe %>%
  head
```

##  Fitting a line {.smaller}
```{r eval = FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(y1, x1)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

##  Fitting a line {.smaller}

```{r echo=FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(y1, x1)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

# Data visualization with `ggplot2`

## Why visualise?

- We can process a lot of information quickly with our eyes
- Plots give us information about
    - Distribution / shape
    - Irregularities
    - Assumptions
    - Intuitions
- Summary statistics, correlations, parameters, model tests, *p*-values do not tell the whole story

### ALWAYS plot your data!


## Why visualise?

<img src="anscombe.svg" style="display:block;width:90%;margin:0 auto;"></img>
<p style="text-align:center;font-style:italic;font-size:0.5em;">Source: Anscombe, F. J. (1973). "Graphs in Statistical Analysis". American Statistician. 27 (1): 17â€“21.</p>


## Why visualise?

<img src="datasaurus.gif" style="display:block;width:90%;margin:0 auto;"></img>

## What is `ggplot2`?
Layered plotting based on the book **The Grammer of Graphics** by Leland Wilkinsons.

With `ggplot2` you

1. provide the _data_
2. define how to map variables to _aesthetics_
3. state which _geometric object_ to display
4. (optional) edit the overall _theme_ of the plot

`ggplot2` then takes care of the details

## An example: scatterplot

1: Provide the data
```{r, eval=FALSE}
boys %>%
  ggplot()
```

2: map variable to aesthetics
```{r, eval=FALSE}
boys %>%
  ggplot(aes(x = age, y = bmi))
```

3: state which geometric object to display
```{r, eval=FALSE}
boys %>%
  ggplot(aes(x = age, y = bmi)) +
  geom_point()
```

## An example: scatterplot
```{r, echo=FALSE, fig.align='center'}
boys %>%
  ggplot(aes(x = age, y = bmi)) +
  geom_point(na.rm = TRUE)
```

## Why this syntax?

Create the plot
```{r, fig.align='center', dev.args=list(bg="transparent"), warning=FALSE, message=FALSE}
gg <- 
  boys %>%
  ggplot(aes(x = age, y = bmi)) +
  geom_point(col = "dark green")
```

Add another layer (smooth fit line)
```{r, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
gg <- gg + 
  geom_smooth(col = "dark blue")
```

Give it some labels and a nice look
```{r, fig.align='center', dev.args=list(bg="transparent")}
gg <- gg + 
  labs(x = "Age", y = "BMI", title = "BMI trend for boys") +
  theme_minimal()
```

## Why this syntax?
```{r, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
plot(gg)
```

## Why this syntax?
<img src="ggani.gif" style="display:block;width:90%;margin:0 auto;"></img>

# Back to `lm`

##  Fitting a line {.smaller}
The linear model would take the following form:
```{r eval=FALSE}
fit <- 
  yourdata %>%
  lm(youroutcome ~ yourpredictors)

fit %>% summary() # pipe
summary(fit) # base R
```
Output:

-  Residuals: minimum, maximum and quartiles
-  Coefficients: estimates, SE's, t-values and $p$-values
-  Fit measures
    -  Residuals SE (standard error residuals)
    -  Multiple R-squared (proportion variance explained)
    -  F-statistic and $p$-value (significance test model)

##  `anscombe` example {.smaller}
```{r message=FALSE, warning = FALSE}
fit <- anscombe %$%
  lm(y1 ~ x1)

fit %>% summary
```

## Checking assumptions

1. linearity
    - scatterplot $y$ and $x$ 
    - include loess curve when in doubt
    - does a squared term improve fit?
2. normality residuals
    -  normal probability plots `qqnorm()`
    -  if sample is small; `qqnorm` with simulated errors cf. `rnorm(n, 0, s)` 
3. constant error variance 
    -  residual plot
    -  scale-location plot

## Linearity {.smaller}
```{r eval = FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(x1, y1)) + 
  geom_point() + 
  geom_smooth(method = "loess", col = "blue") + 
  geom_smooth(method = "lm", col = "orange")
```

## Linearity {.smaller}
```{r echo=FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(x1, y1)) + 
  geom_point() + 
  geom_smooth(method = "loess", col = "blue") + 
  geom_smooth(method = "lm", col = "orange")
```

The loess curve suggests slight non-linearity

## Adding a squared term
```{r message=FALSE, warning = FALSE}
anscombe %$%
  lm(y1 ~ x1 + I(x1^2)) %>%
  summary()
```

## Constant error variance? {.smaller}
```{r, fig.height=4, dev.args = list(bg = 'transparent')}
par(mfrow = c(1, 2))
fit %>%
  plot(which = c(1, 3), cex = .6)
```

## No constant error variance! {.smaller}
```{r, fig.height=4, dev.args = list(bg = 'transparent')}
par(mfrow = c(1, 2))
boys %$%
  lm(bmi ~ age) %>%
  plot(which = c(1, 3), cex = .6)
```

## Normality of errors {.smaller}
```{r, fig.height=4, dev.args = list(bg = 'transparent')}
fit %>%
  plot(which = 2, cex = .6)
```

The QQplot shows some divergence from normality at the tails

# Outliers, influence and robust regression

## Outliers and influential cases {.smaller}

Leverage: see the fit line as a lever. 
  
  - some points pull/push harder; they have more leverage
  
Standardized residuals:

  - The values that have more leverage tend to be closer to the line
  - The line is fit so as to be closer to them
  - The residual standard deviation can differ at different points on $X$ - even if the error standard deviation is constant. 
  - Therefore we standardize the residuals so that they have constant variance (assuming homoscedasticity). 

Cook's distance: how far the predicted values would move if your model were fit without the data point in question. 

  - it is a function of the leverage and standardized residual  associated with each data point

```{r echo = FALSE}
set.seed(20)

pred1 = rnorm(20, mean=20, sd=3)
outcome1 = 5 + .5*pred1 + rnorm(20)

pred2 = c(pred1, 30);        outcome2 = c(outcome1, 20.8)
pred3 = c(pred1, 19.44);     outcome3 = c(outcome1, 20.8)
pred4 = c(pred1, 30);        outcome4 = c(outcome1, 10)
```

## Fine
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome1 ~ pred1, ylim = c(9, 25), xlim = c(10, 30))
abline(lm(outcome1 ~ pred1))
plot(lm(outcome1 ~ pred1), which = 5)
```

## High leverage, low residual
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome2 ~ pred2, ylim = c(9, 25), xlim = c(10, 30))
points(30, 20.8, col = "red")
abline(lm(outcome2 ~ pred2))
plot(lm(outcome2 ~ pred2), which = 5)
```

## Low leverage, high residual
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome3 ~ pred3, ylim = c(9, 25), xlim = c(10, 30))
abline(lm(outcome3 ~ pred3))
points(19.44, 20.8, col = "red")
plot(lm(outcome3 ~ pred3), which = 5)
```

## High leverage, high residual
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome4 ~ pred4, ylim = c(9, 25), xlim = c(10, 30))
points(30, 10, col = "red")
abline(lm(outcome4 ~ pred4))
plot(lm(outcome4 ~ pred4), which = 5)
```

## Outliers and influential cases
Outliers are cases with large $e_z$  (standardized residuals).

If the model is ***correct***  we expect:
  
  -  5\% of $|e_z|>1.96$ 
  -  1\% of $|e_z|>2.58$
  -  0\% of $|e_z|>3.3$ 

Influential cases are cases with large influence on parameter estimates
  
  -  cases with Cook's Distance $> 1$, or 
  -  cases with Cook's Distance much larger than the rest

## Outliers and influential cases
```{r, fig.height= 3, dev.args = list(bg = 'transparent')}
par(mfrow = c(1, 2), cex = .6)
fit %>% plot(which = c(4, 5))
```

There are no cases with $|e_z|>2$, so no outliers (right plot). There are no cases with Cook's Distance $>1$, but case 3 stands out 

## Next week

- Inferential modeling
  - drawing conclusions from data
  - confidence intervals

- What if assumptions are violated?